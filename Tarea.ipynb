{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartaCaste/Master-Big-Data/blob/main/Tarea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prospective-america",
      "metadata": {
        "id": "prospective-america"
      },
      "source": [
        "Dado que el entrenamiento de redes neuronales es una tarea  muy costosa, **se recomienda ejecutar el notebooks en [Google Colab](https://colab.research.google.com)**, por supuesto también se puede ejecutar en local.\n",
        "\n",
        "Al entrar en [Google Colab](https://colab.research.google.com) bastará con hacer click en `upload` y subir este notebook. No olvide luego descargarlo en `File->Download .ipynb`\n",
        "\n",
        "**El examen deberá ser entregado con las celdas ejecutadas, si alguna celda no está ejecutadas no se contará.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "novel-stewart",
      "metadata": {
        "id": "novel-stewart"
      },
      "source": [
        "El examen se divide en tres partes, con la puntuación que se indica a continuación. La puntuación máxima será 10.\n",
        "\n",
        "- [Actividad 1: Redes Densas](#actividad_1): 5.5 pts\n",
        "    - Correcta normalización: máximo de 0.5 pts\n",
        "    - [Cuestión 1](#1.1): 1 pt\n",
        "    - [Cuestión 2](#1.2): 1 pt\n",
        "    - [Cuestión 3](#1.3): 0.5 pts\n",
        "    - [Cuestión 4](#1.4): 0.5 pts\n",
        "    - [Cuestión 5](#1.5): 0.5 pts\n",
        "    - [Cuestión 6](#1.6): 0.5 pts\n",
        "    - [Cuestión 7](#1.7): 0.5 pts\n",
        "    - [Cuestión 8](#1.8): 0.5 pts\n",
        "\n",
        "\n",
        "- [Actividad 2: Redes Convolucionales](#actividad_2): 4.5 pts\n",
        "    - [Cuestión 1](#2.1): 1 pt\n",
        "    - [Cuestión 2](#2.2): 1.5 pt\n",
        "    - [Cuestión 3](#2.3): 0.5 pts\n",
        "    - [Cuestión 4](#2.4): 0.5 pts\n",
        "    - [Cuestión 5](#2.5): 0.5 pts\n",
        "    - [Cuestión 6](#2.6): 0.5 pts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "prompt-developer",
      "metadata": {
        "id": "prompt-developer"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocal-correction",
      "metadata": {
        "id": "vocal-correction"
      },
      "source": [
        "<a name='actividad_1'></a>\n",
        "# Actividad 1: Redes Densas\n",
        "\n",
        "Para esta primera actividad vamos a utilizar el [boston housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). Con el que trataremos de predecir el precio de una casa con 13 features.\n",
        "\n",
        "**Puntuación**: \n",
        "\n",
        "Normalizar las features correctamente (x_train, x_test): 0.5 pts \n",
        "\n",
        "- Correcta normalización: máximo de 0.5 pts\n",
        "- [Cuestión 1](#1.1): 1 pt\n",
        "- [Cuestión 2](#1.2): 1 pt\n",
        "- [Cuestión 3](#1.3): 0.5 pts\n",
        "- [Cuestión 4](#1.4): 0.5 pts\n",
        "- [Cuestión 5](#1.5): 0.5 pts\n",
        "- [Cuestión 6](#1.6): 0.5 pts\n",
        "- [Cuestión 7](#1.7): 0.5 pts\n",
        "- [Cuestión 8](#1.8): 0.5 pts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "presidential-milan",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "presidential-milan",
        "outputId": "362dd42f-8b33-48b7-a7e9-a39e8c5a63c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train, y_train shapes: (404, 13) (404,)\n",
            "x_test, y_test shapes: (404, 13) (404,)\n",
            "Some prices:  [15.2 42.3 50.  21.1 17.7]\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path='boston_housing.npz',\n",
        "    test_split=0.2,\n",
        ")\n",
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_train.shape, y_train.shape)\n",
        "print('Some prices: ', y_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = x_train.mean(axis=0)\n",
        "std = x_train.std(axis=0)\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = (x_test - mean) / std"
      ],
      "metadata": {
        "id": "kW-e3gg_xN88"
      },
      "id": "kW-e3gg_xN88",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "underlying-planner",
      "metadata": {
        "id": "underlying-planner"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "## Cuestión 1: Cree un modelo secuencial que contenga 4 capas ocultas(hidden layers), con más de 60 neuronas  por capa, sin regularización y obtenga los resultados.\n",
        "\n",
        "Puntuación: \n",
        "- Obtener el modelo correcto: 0.8 pts\n",
        "- Compilar el modelo: 0.1pts\n",
        "- Acertar con la función de pérdida: 0.1 pts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "working-shade",
      "metadata": {
        "id": "working-shade"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Dense(100, input_shape=(13, ), activation='relu', name='hidden_layer_1'))\n",
        "\n",
        "model.add(layers.Dense(90, activation='relu', name='hidden_layer_2'))\n",
        "\n",
        "model.add(layers.Dense(80, activation='relu', name='hidden_layer_3'))\n",
        "\n",
        "model.add(layers.Dense(70, activation='relu', name='hidden_layer_4'))\n",
        "\n",
        "model.add(layers.Dense(1, activation='linear', name='layer_output'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "mobile-change",
      "metadata": {
        "id": "mobile-change"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "rotary-credits",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rotary-credits",
        "outputId": "5b473abc-443f-4f18-8f72-d09c06ef4324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.6180 - mae: 0.9384 - val_loss: 12.4015 - val_mae: 2.3909\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3149 - mae: 0.8105 - val_loss: 11.6567 - val_mae: 2.3111\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2004 - mae: 0.7834 - val_loss: 12.6012 - val_mae: 2.3816\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2547 - mae: 0.8092 - val_loss: 11.7854 - val_mae: 2.2537\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1942 - mae: 0.7794 - val_loss: 11.9750 - val_mae: 2.3095\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.0121 - mae: 0.7050 - val_loss: 11.6963 - val_mae: 2.3078\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0344 - mae: 0.7082 - val_loss: 12.0683 - val_mae: 2.3316\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0049 - mae: 0.6833 - val_loss: 12.3299 - val_mae: 2.3658\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.0952 - mae: 0.7398 - val_loss: 12.5430 - val_mae: 2.3511\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.0491 - mae: 0.7064 - val_loss: 12.4765 - val_mae: 2.3634\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.0313 - mae: 0.7225 - val_loss: 12.1087 - val_mae: 2.3325\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1636 - mae: 0.7677 - val_loss: 12.2097 - val_mae: 2.3364\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.4382 - mae: 0.8093 - val_loss: 11.7757 - val_mae: 2.3074\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3665 - mae: 0.8562 - val_loss: 13.0006 - val_mae: 2.4164\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.4430 - mae: 0.8817 - val_loss: 11.8164 - val_mae: 2.3298\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.5140 - mae: 0.9010 - val_loss: 12.3053 - val_mae: 2.3661\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2270 - mae: 0.8253 - val_loss: 12.5306 - val_mae: 2.3737\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.5610 - mae: 0.8705 - val_loss: 12.8131 - val_mae: 2.4145\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.9692 - mae: 1.0170 - val_loss: 11.1568 - val_mae: 2.3318\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.1505 - mae: 1.0648 - val_loss: 13.3397 - val_mae: 2.4183\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6058 - mae: 0.9395 - val_loss: 12.0415 - val_mae: 2.3741\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2653 - mae: 0.8287 - val_loss: 13.0996 - val_mae: 2.4113\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.1014 - mae: 0.7493 - val_loss: 13.5088 - val_mae: 2.4538\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1899 - mae: 0.7944 - val_loss: 12.9572 - val_mae: 2.4092\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3080 - mae: 0.8345 - val_loss: 11.8153 - val_mae: 2.3108\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2529 - mae: 0.8248 - val_loss: 14.3398 - val_mae: 2.5779\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.6857 - mae: 0.9827 - val_loss: 13.2715 - val_mae: 2.4584\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2530 - mae: 0.8250 - val_loss: 11.1215 - val_mae: 2.2293\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2557 - mae: 0.8026 - val_loss: 12.9341 - val_mae: 2.4063\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1828 - mae: 1.0742 - val_loss: 14.1651 - val_mae: 2.5197\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4085 - mae: 0.8810 - val_loss: 13.2047 - val_mae: 2.5559\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.2171 - mae: 0.8311 - val_loss: 12.8427 - val_mae: 2.4324\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0509 - mae: 1.1002 - val_loss: 15.1423 - val_mae: 2.7553\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 3.4281 - mae: 1.4354 - val_loss: 13.9929 - val_mae: 2.5094\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9622 - mae: 1.1153 - val_loss: 12.6068 - val_mae: 2.3981\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3065 - mae: 0.8398 - val_loss: 12.8596 - val_mae: 2.4093\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2157 - mae: 0.8020 - val_loss: 12.3679 - val_mae: 2.3822\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0637 - mae: 0.7468 - val_loss: 11.6695 - val_mae: 2.2859\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0157 - mae: 0.7151 - val_loss: 12.2261 - val_mae: 2.3368\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0385 - mae: 0.7662 - val_loss: 11.6016 - val_mae: 2.2610\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9322 - mae: 0.6736 - val_loss: 12.6762 - val_mae: 2.3694\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9768 - mae: 0.7239 - val_loss: 12.6039 - val_mae: 2.3786\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.2637 - mae: 0.7984 - val_loss: 11.5836 - val_mae: 2.3527\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.4413 - mae: 0.9079 - val_loss: 13.8642 - val_mae: 2.4867\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3042 - mae: 0.8273 - val_loss: 12.2318 - val_mae: 2.3216\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2876 - mae: 0.8142 - val_loss: 11.9456 - val_mae: 2.3327\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9496 - mae: 0.7064 - val_loss: 12.2329 - val_mae: 2.3462\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0523 - mae: 0.7317 - val_loss: 12.8170 - val_mae: 2.4438\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9292 - mae: 0.6854 - val_loss: 12.7369 - val_mae: 2.4022\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.7395 - mae: 0.9790 - val_loss: 13.3721 - val_mae: 2.5310\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.5541 - mae: 0.9365 - val_loss: 13.3035 - val_mae: 2.4421\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.5811 - mae: 0.9629 - val_loss: 14.0531 - val_mae: 2.5908\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4019 - mae: 0.8864 - val_loss: 11.6922 - val_mae: 2.3199\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9483 - mae: 0.7008 - val_loss: 13.3042 - val_mae: 2.5052\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3294 - mae: 0.8669 - val_loss: 12.6338 - val_mae: 2.3877\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9458 - mae: 0.6976 - val_loss: 12.4525 - val_mae: 2.4082\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.9962 - mae: 0.6957 - val_loss: 12.2394 - val_mae: 2.4038\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.9067 - mae: 0.6648 - val_loss: 12.1477 - val_mae: 2.3605\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7840 - mae: 0.6320 - val_loss: 12.4586 - val_mae: 2.3427\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.8875 - mae: 0.7010 - val_loss: 12.8552 - val_mae: 2.4204\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.8802 - mae: 0.6924 - val_loss: 11.8993 - val_mae: 2.3206\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7889 - mae: 0.6382 - val_loss: 12.6825 - val_mae: 2.3954\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7972 - mae: 0.6294 - val_loss: 11.9868 - val_mae: 2.3243\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7838 - mae: 0.6159 - val_loss: 13.4790 - val_mae: 2.4754\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.6395 - mae: 0.9382 - val_loss: 14.8815 - val_mae: 2.7060\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.2072 - mae: 1.1803 - val_loss: 11.9082 - val_mae: 2.4110\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.5117 - mae: 0.9552 - val_loss: 13.1682 - val_mae: 2.4694\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.2498 - mae: 0.8381 - val_loss: 12.7615 - val_mae: 2.4433\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0872 - mae: 0.7587 - val_loss: 11.8216 - val_mae: 2.2353\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9280 - mae: 0.6952 - val_loss: 13.2260 - val_mae: 2.4383\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7550 - mae: 0.6382 - val_loss: 12.8372 - val_mae: 2.4412\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.9086 - mae: 0.6891 - val_loss: 13.4573 - val_mae: 2.4908\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9970 - mae: 0.7315 - val_loss: 12.8244 - val_mae: 2.4028\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.8993 - mae: 0.6963 - val_loss: 13.6756 - val_mae: 2.4655\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9538 - mae: 0.6987 - val_loss: 13.6196 - val_mae: 2.4666\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mae: 0.6660 - val_loss: 12.2957 - val_mae: 2.3541\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9906 - mae: 0.7347 - val_loss: 12.0024 - val_mae: 2.2718\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.0293 - mae: 0.7096 - val_loss: 13.6760 - val_mae: 2.6004\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.8270 - mae: 0.9750 - val_loss: 15.4763 - val_mae: 2.6981\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2253 - mae: 0.8129 - val_loss: 13.7105 - val_mae: 2.4977\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9619 - mae: 0.7171 - val_loss: 13.0062 - val_mae: 2.3840\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.8319 - mae: 0.6598 - val_loss: 12.2510 - val_mae: 2.3514\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.8840 - mae: 0.6773 - val_loss: 13.1014 - val_mae: 2.4210\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7492 - mae: 0.6093 - val_loss: 13.3508 - val_mae: 2.4382\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6514 - mae: 0.5672 - val_loss: 12.8682 - val_mae: 2.3669\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.7490 - mae: 0.6345 - val_loss: 12.5888 - val_mae: 2.3750\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.9043 - mae: 0.6739 - val_loss: 13.8829 - val_mae: 2.4977\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.7135 - mae: 0.6039 - val_loss: 12.9784 - val_mae: 2.4289\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.0419 - mae: 0.7159 - val_loss: 13.4806 - val_mae: 2.4767\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.7308 - mae: 0.6387 - val_loss: 13.1138 - val_mae: 2.4267\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6887 - mae: 0.6098 - val_loss: 12.6361 - val_mae: 2.3739\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9507 - mae: 0.7084 - val_loss: 12.9944 - val_mae: 2.4107\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9502 - mae: 0.6942 - val_loss: 14.4790 - val_mae: 2.5361\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.7991 - mae: 0.6429 - val_loss: 13.1026 - val_mae: 2.4091\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6652 - mae: 0.5875 - val_loss: 13.0113 - val_mae: 2.4289\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.6956 - mae: 0.5932 - val_loss: 13.4993 - val_mae: 2.4380\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5967 - mae: 0.5483 - val_loss: 12.8969 - val_mae: 2.4120\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6212 - mae: 0.5814 - val_loss: 12.4128 - val_mae: 2.3519\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5890 - mae: 0.5510 - val_loss: 13.0702 - val_mae: 2.4434\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5629 - mae: 0.5475 - val_loss: 12.6087 - val_mae: 2.3908\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5684 - mae: 0.5276 - val_loss: 12.7544 - val_mae: 2.3789\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.8788 - mae: 0.6854 - val_loss: 13.9305 - val_mae: 2.5290\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.8407 - mae: 0.6853 - val_loss: 14.0278 - val_mae: 2.4826\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.0259 - mae: 0.7488 - val_loss: 12.7943 - val_mae: 2.4039\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9313 - mae: 0.7317 - val_loss: 12.3270 - val_mae: 2.3438\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.6245 - mae: 1.2174 - val_loss: 15.1967 - val_mae: 2.6543\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.6594 - mae: 0.9277 - val_loss: 13.2125 - val_mae: 2.4222\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.0635 - mae: 0.7855 - val_loss: 12.1694 - val_mae: 2.4018\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.7382 - mae: 0.6535 - val_loss: 13.1051 - val_mae: 2.4280\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.7567 - mae: 0.6255 - val_loss: 12.7606 - val_mae: 2.3445\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7223 - mae: 0.5872 - val_loss: 12.8112 - val_mae: 2.3730\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5450 - mae: 0.5098 - val_loss: 13.1100 - val_mae: 2.3990\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.8672 - mae: 0.6809 - val_loss: 12.8479 - val_mae: 2.3917\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6716 - mae: 0.5982 - val_loss: 13.4517 - val_mae: 2.4401\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.7410 - mae: 0.6698 - val_loss: 14.5449 - val_mae: 2.5719\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6580 - mae: 0.5971 - val_loss: 12.6368 - val_mae: 2.3325\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6762 - mae: 0.5941 - val_loss: 13.6725 - val_mae: 2.4417\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5840 - mae: 0.5581 - val_loss: 13.4346 - val_mae: 2.4453\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6424 - mae: 0.5736 - val_loss: 13.2359 - val_mae: 2.4185\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.5674 - mae: 0.5226 - val_loss: 13.3163 - val_mae: 2.4422\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6496 - mae: 0.5609 - val_loss: 12.4153 - val_mae: 2.3591\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5673 - mae: 0.5418 - val_loss: 13.1317 - val_mae: 2.4444\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4860 - mae: 0.4853 - val_loss: 13.0719 - val_mae: 2.4403\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4584 - mae: 0.4685 - val_loss: 13.2009 - val_mae: 2.4505\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5639 - mae: 0.5436 - val_loss: 12.9810 - val_mae: 2.4071\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6130 - mae: 0.5606 - val_loss: 13.4636 - val_mae: 2.4216\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5691 - mae: 0.5324 - val_loss: 13.8537 - val_mae: 2.4850\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4344 - mae: 0.4534 - val_loss: 13.5991 - val_mae: 2.4625\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.8476 - mae: 0.6437 - val_loss: 12.6345 - val_mae: 2.3999\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.8586 - mae: 0.6762 - val_loss: 13.3761 - val_mae: 2.4141\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6990 - mae: 0.6089 - val_loss: 12.7224 - val_mae: 2.3690\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6208 - mae: 0.5749 - val_loss: 12.9230 - val_mae: 2.3869\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4976 - mae: 0.5249 - val_loss: 13.5084 - val_mae: 2.4303\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5182 - mae: 0.5073 - val_loss: 13.0373 - val_mae: 2.3913\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6622 - mae: 0.5746 - val_loss: 13.7085 - val_mae: 2.5190\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.8410 - mae: 0.7212 - val_loss: 14.1396 - val_mae: 2.4925\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.6699 - mae: 0.5991 - val_loss: 14.2597 - val_mae: 2.5250\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5471 - mae: 0.5532 - val_loss: 13.5298 - val_mae: 2.4605\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9226 - mae: 0.6914 - val_loss: 13.9290 - val_mae: 2.5283\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.6749 - mae: 0.6122 - val_loss: 14.2796 - val_mae: 2.5532\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.6032 - mae: 0.5599 - val_loss: 13.4203 - val_mae: 2.4447\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.6030 - mae: 0.5719 - val_loss: 12.8213 - val_mae: 2.3845\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.5858 - mae: 0.5600 - val_loss: 13.5838 - val_mae: 2.5089\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.7872 - mae: 0.6944 - val_loss: 13.7291 - val_mae: 2.5053\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 0.9243 - mae: 0.7449 - val_loss: 13.0498 - val_mae: 2.4976\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 0.8366 - mae: 0.6707 - val_loss: 13.4329 - val_mae: 2.4447\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.7287 - mae: 0.6401 - val_loss: 12.8555 - val_mae: 2.4348\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.6792 - mae: 0.5968 - val_loss: 13.6452 - val_mae: 2.4450\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.5143 - mae: 0.5095 - val_loss: 13.5915 - val_mae: 2.4816\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 0.5842 - mae: 0.5627 - val_loss: 13.9438 - val_mae: 2.5002\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 0.6232 - mae: 0.5564 - val_loss: 13.3610 - val_mae: 2.4269\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 0.4949 - mae: 0.5269 - val_loss: 13.7248 - val_mae: 2.4433\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 0.4305 - mae: 0.4623 - val_loss: 13.0622 - val_mae: 2.3810\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.5314 - mae: 0.5404 - val_loss: 13.3519 - val_mae: 2.4410\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.4500 - mae: 0.4828 - val_loss: 13.8920 - val_mae: 2.5802\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 0.9495 - mae: 0.6878 - val_loss: 15.0244 - val_mae: 2.6771\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.9755 - mae: 0.7585 - val_loss: 13.5113 - val_mae: 2.4627\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 0.6597 - mae: 0.5898 - val_loss: 12.9429 - val_mae: 2.3479\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.5451 - mae: 0.5177 - val_loss: 13.0969 - val_mae: 2.4717\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.4220 - mae: 0.8248 - val_loss: 16.3283 - val_mae: 2.8031\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.2340 - mae: 0.8792 - val_loss: 14.0068 - val_mae: 2.5595\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9752 - mae: 0.7220 - val_loss: 14.4802 - val_mae: 2.5981\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.7811 - mae: 0.6334 - val_loss: 14.4026 - val_mae: 2.5105\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.8061 - mae: 0.6469 - val_loss: 13.1327 - val_mae: 2.4476\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.7978 - mae: 0.6323 - val_loss: 12.5234 - val_mae: 2.3405\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5656 - mae: 0.5679 - val_loss: 13.5609 - val_mae: 2.4900\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.8445 - mae: 0.6981 - val_loss: 12.8897 - val_mae: 2.4552\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.9063 - mae: 0.6604 - val_loss: 12.9081 - val_mae: 2.3501\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6688 - mae: 0.6060 - val_loss: 13.5999 - val_mae: 2.5088\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4970 - mae: 0.5134 - val_loss: 14.0912 - val_mae: 2.4878\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4040 - mae: 0.4399 - val_loss: 12.9620 - val_mae: 2.4151\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5174 - mae: 0.4776 - val_loss: 13.3455 - val_mae: 2.4471\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4514 - mae: 0.4868 - val_loss: 12.8520 - val_mae: 2.4091\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4319 - mae: 0.4557 - val_loss: 13.1699 - val_mae: 2.4585\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.7899 - mae: 0.6346 - val_loss: 13.2631 - val_mae: 2.4636\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6486 - mae: 0.5743 - val_loss: 13.2375 - val_mae: 2.4605\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5249 - mae: 0.4997 - val_loss: 12.9858 - val_mae: 2.4067\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.5035 - mae: 0.5120 - val_loss: 13.2764 - val_mae: 2.4114\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6365 - mae: 0.5675 - val_loss: 13.3252 - val_mae: 2.4521\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6110 - mae: 0.5520 - val_loss: 13.6302 - val_mae: 2.5013\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5687 - mae: 0.5644 - val_loss: 13.0612 - val_mae: 2.3899\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4940 - mae: 0.5040 - val_loss: 13.1285 - val_mae: 2.4560\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.4257 - mae: 0.4571 - val_loss: 13.5442 - val_mae: 2.4788\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3556 - mae: 0.4160 - val_loss: 13.8615 - val_mae: 2.4802\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5272 - mae: 0.4921 - val_loss: 13.4535 - val_mae: 2.4812\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5267 - mae: 0.5105 - val_loss: 13.8284 - val_mae: 2.4514\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4311 - mae: 0.4793 - val_loss: 14.2685 - val_mae: 2.5773\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5080 - mae: 0.5255 - val_loss: 13.2755 - val_mae: 2.4580\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4198 - mae: 0.4716 - val_loss: 13.5425 - val_mae: 2.4433\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4206 - mae: 0.4680 - val_loss: 13.9393 - val_mae: 2.5067\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4404 - mae: 0.4608 - val_loss: 14.0918 - val_mae: 2.5523\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3630 - mae: 0.4229 - val_loss: 13.4841 - val_mae: 2.4223\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4563 - mae: 0.4669 - val_loss: 14.2556 - val_mae: 2.5251\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5052 - mae: 0.5231 - val_loss: 14.0845 - val_mae: 2.5125\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6243 - mae: 0.5980 - val_loss: 14.9571 - val_mae: 2.6433\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6084 - mae: 0.5726 - val_loss: 13.7063 - val_mae: 2.5013\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.5304 - mae: 0.5440 - val_loss: 14.8006 - val_mae: 2.5675\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5470 - mae: 0.4861 - val_loss: 13.5537 - val_mae: 2.4630\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3831 - mae: 0.4462 - val_loss: 13.2493 - val_mae: 2.4256\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.9300 - mae: 0.7321 - val_loss: 15.0021 - val_mae: 2.7091\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c3d0eacd0>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=32,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "descending-letters",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "descending-letters",
        "outputId": "898be5c5-8473-4666-bda9-5b162097d9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 21.9365 - mae: 3.1997\n",
            "Test Loss: [21.936532974243164, 3.199667453765869]\n"
          ]
        }
      ],
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "pred_train= model.predict(x_train)\n",
        "print(np.sqrt(mean_squared_error(y_train,pred_train)))\n",
        "\n",
        "pred= model.predict(x_test)\n",
        "print(np.sqrt(mean_squared_error(y_test,pred))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OLi9EUoLRSl",
        "outputId": "859aa453-606e-4f05-a78e-199c280b5e57"
      },
      "id": "5OLi9EUoLRSl",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0498569745871427\n",
            "4.683645379910774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raised-delivery",
      "metadata": {
        "id": "raised-delivery"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "## Cuestión 2: Utilice el mismo modelo de la cuestión anterior pero añadiendo al menos dos técnicas distinas de regularización.\n",
        "\n",
        "Ejemplos de regularización: [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb)\n",
        "\n",
        "Puntuación:\n",
        "\n",
        "- Obtener el modelo con la regularización: 0.8 pts\n",
        "- Obtener un `test loss` inferior al anterior: 0.2 pts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tecnicas de Regularización**\n",
        "\n",
        "A continuación añadimos como técnicas de regularización un dropout de 0.3, una regularización L2 y una reducción del batch_size a 15."
      ],
      "metadata": {
        "id": "SiMleWZVeboL"
      },
      "id": "SiMleWZVeboL"
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "kernel_regularizer_l2 = keras.regularizers.l2(5e-4)\n",
        "\n",
        "## Hidden Layer + regularization\n",
        "model.add(keras.layers.Dense(100, input_shape=(13, ), activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_1'))\n",
        "\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(90, activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_2'))\n",
        "\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(80, activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_3'))\n",
        "\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(70, activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_4'))\n",
        "\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "### output layer\n",
        "model.add(layers.Dense(1, activation='linear', name='layer_output'))\n"
      ],
      "metadata": {
        "id": "JRzI9y_TVHOs"
      },
      "id": "JRzI9y_TVHOs",
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "focal-traffic",
      "metadata": {
        "id": "focal-traffic"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "338f8622",
      "metadata": {
        "id": "338f8622"
      },
      "outputs": [],
      "source": [
        "batch_size=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "prostate-instrumentation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prostate-instrumentation",
        "outputId": "45eb96eb-5822-475a-8f52-7e749069c97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "22/22 [==============================] - 1s 14ms/step - loss: 510.2296 - mae: 20.6649 - val_loss: 468.9520 - val_mae: 19.6524\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 210.3999 - mae: 11.6787 - val_loss: 77.4098 - val_mae: 6.5716\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 84.3061 - mae: 6.9485 - val_loss: 64.0720 - val_mae: 5.7883\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 68.6467 - mae: 6.1089 - val_loss: 34.0979 - val_mae: 4.0284\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 58.4658 - mae: 5.6526 - val_loss: 36.2175 - val_mae: 4.2843\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 50.3415 - mae: 5.4326 - val_loss: 43.4932 - val_mae: 4.8647\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 54.8392 - mae: 5.4804 - val_loss: 19.0043 - val_mae: 3.0792\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 45.4640 - mae: 5.0447 - val_loss: 22.3163 - val_mae: 3.3762\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 48.3387 - mae: 5.2715 - val_loss: 16.7176 - val_mae: 3.0425\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 43.3214 - mae: 4.9215 - val_loss: 20.8449 - val_mae: 3.4831\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 39.6129 - mae: 4.8538 - val_loss: 26.3528 - val_mae: 4.0255\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 38.7695 - mae: 4.7729 - val_loss: 29.6118 - val_mae: 4.2274\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 39.9472 - mae: 4.6489 - val_loss: 18.8494 - val_mae: 3.2749\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 35.4837 - mae: 4.5437 - val_loss: 13.8839 - val_mae: 2.8499\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 33.6004 - mae: 4.3030 - val_loss: 21.4842 - val_mae: 3.6874\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 38.0714 - mae: 4.5584 - val_loss: 18.5244 - val_mae: 3.4015\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 31.7015 - mae: 4.2626 - val_loss: 14.2192 - val_mae: 2.9035\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 30.9441 - mae: 4.3664 - val_loss: 15.8291 - val_mae: 3.1685\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 33.8177 - mae: 4.3855 - val_loss: 15.8566 - val_mae: 3.1916\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 29.4261 - mae: 4.2402 - val_loss: 15.9202 - val_mae: 3.2213\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 31.1168 - mae: 4.1980 - val_loss: 13.5343 - val_mae: 2.8651\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 30.8948 - mae: 4.1782 - val_loss: 16.1127 - val_mae: 3.1332\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 26.4203 - mae: 3.7590 - val_loss: 13.9957 - val_mae: 2.8442\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 31.8869 - mae: 4.2033 - val_loss: 17.3225 - val_mae: 3.3689\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 27.5149 - mae: 3.9660 - val_loss: 20.4736 - val_mae: 3.6519\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 29.2356 - mae: 4.0303 - val_loss: 26.1151 - val_mae: 4.2272\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 26.8294 - mae: 3.9214 - val_loss: 19.3760 - val_mae: 3.5787\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 29.7850 - mae: 4.1612 - val_loss: 19.2357 - val_mae: 3.5679\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 28.4027 - mae: 4.0587 - val_loss: 21.9874 - val_mae: 3.8290\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 29.7579 - mae: 4.0502 - val_loss: 12.4043 - val_mae: 2.8122\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 28.0745 - mae: 3.9511 - val_loss: 17.6231 - val_mae: 3.3285\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 34.1571 - mae: 4.3686 - val_loss: 17.9932 - val_mae: 3.3272\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 33.2862 - mae: 4.1202 - val_loss: 17.3876 - val_mae: 3.2373\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 36.3244 - mae: 4.5048 - val_loss: 17.5635 - val_mae: 3.2948\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 30.0072 - mae: 4.1394 - val_loss: 15.7451 - val_mae: 3.0891\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 25.6865 - mae: 3.7229 - val_loss: 12.9462 - val_mae: 2.7266\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 28.7184 - mae: 3.8919 - val_loss: 12.7283 - val_mae: 2.8419\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 27.0289 - mae: 3.7705 - val_loss: 14.1549 - val_mae: 2.9359\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 28.5740 - mae: 3.9128 - val_loss: 17.7406 - val_mae: 3.3256\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 27.1946 - mae: 3.7891 - val_loss: 17.1124 - val_mae: 3.2690\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 27.2344 - mae: 3.7273 - val_loss: 17.5549 - val_mae: 3.2859\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 26.9342 - mae: 3.9551 - val_loss: 16.9982 - val_mae: 3.1867\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 26.6148 - mae: 3.8307 - val_loss: 15.9770 - val_mae: 3.1823\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 23.8943 - mae: 3.6943 - val_loss: 13.9356 - val_mae: 2.8951\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 27.6000 - mae: 3.8719 - val_loss: 15.1775 - val_mae: 2.9248\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 25.7591 - mae: 3.8403 - val_loss: 19.6952 - val_mae: 3.6173\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 24.3446 - mae: 3.7769 - val_loss: 14.5926 - val_mae: 3.1231\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.7518 - mae: 3.6609 - val_loss: 14.5611 - val_mae: 3.0430\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.1864 - mae: 3.4035 - val_loss: 19.0208 - val_mae: 3.4231\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 27.1948 - mae: 3.8092 - val_loss: 13.1566 - val_mae: 2.7701\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.6223 - mae: 3.5160 - val_loss: 17.0328 - val_mae: 3.2469\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.3984 - mae: 3.5455 - val_loss: 12.8011 - val_mae: 2.6367\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.6316 - mae: 3.5472 - val_loss: 13.1758 - val_mae: 2.7566\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 26.8757 - mae: 3.7770 - val_loss: 16.7715 - val_mae: 3.1427\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.1071 - mae: 3.4088 - val_loss: 18.7537 - val_mae: 3.3107\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 25.2282 - mae: 3.7509 - val_loss: 22.3653 - val_mae: 3.6409\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.2902 - mae: 3.4721 - val_loss: 15.9764 - val_mae: 3.1206\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 27.1566 - mae: 3.7308 - val_loss: 11.7849 - val_mae: 2.6947\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 24.6606 - mae: 3.5735 - val_loss: 12.7869 - val_mae: 2.7617\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 22.6908 - mae: 3.5124 - val_loss: 11.3685 - val_mae: 2.7163\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 24.1804 - mae: 3.7030 - val_loss: 11.3958 - val_mae: 2.5624\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 25.5879 - mae: 3.7342 - val_loss: 13.8211 - val_mae: 2.8583\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.9458 - mae: 3.4112 - val_loss: 17.7863 - val_mae: 3.3713\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.2988 - mae: 3.6101 - val_loss: 17.9234 - val_mae: 3.2673\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.9380 - mae: 3.5791 - val_loss: 11.9769 - val_mae: 2.5945\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 28.5268 - mae: 3.9316 - val_loss: 14.7548 - val_mae: 2.8976\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.3201 - mae: 3.4781 - val_loss: 14.5107 - val_mae: 2.8596\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.8610 - mae: 3.6286 - val_loss: 16.6682 - val_mae: 2.9776\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.4427 - mae: 3.6881 - val_loss: 17.1651 - val_mae: 2.9539\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 26.6598 - mae: 3.7041 - val_loss: 15.5804 - val_mae: 2.8517\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 25.3996 - mae: 3.6510 - val_loss: 16.0996 - val_mae: 3.0814\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 22.2484 - mae: 3.5269 - val_loss: 17.6052 - val_mae: 3.1975\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 24.3224 - mae: 3.5092 - val_loss: 18.2817 - val_mae: 3.3315\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.5601 - mae: 3.3380 - val_loss: 18.2644 - val_mae: 3.3448\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.1419 - mae: 3.4473 - val_loss: 13.7534 - val_mae: 2.8682\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.8212 - mae: 3.6629 - val_loss: 15.4326 - val_mae: 2.9447\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 22.3860 - mae: 3.5777 - val_loss: 16.2472 - val_mae: 3.0706\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.9512 - mae: 3.1867 - val_loss: 22.8898 - val_mae: 3.8885\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.4632 - mae: 3.5069 - val_loss: 14.8718 - val_mae: 2.9231\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 25.0985 - mae: 3.6401 - val_loss: 13.8481 - val_mae: 2.7773\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.6825 - mae: 3.3085 - val_loss: 13.7923 - val_mae: 2.7905\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.8703 - mae: 3.3019 - val_loss: 15.0929 - val_mae: 2.8721\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.9090 - mae: 3.3154 - val_loss: 16.1788 - val_mae: 3.0211\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 22.8225 - mae: 3.4964 - val_loss: 13.9357 - val_mae: 2.8395\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.6678 - mae: 3.2022 - val_loss: 9.7866 - val_mae: 2.4211\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.7119 - mae: 3.2544 - val_loss: 10.8788 - val_mae: 2.5178\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.7430 - mae: 3.5381 - val_loss: 8.8589 - val_mae: 2.3272\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.2415 - mae: 3.0369 - val_loss: 13.3405 - val_mae: 2.8530\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.4006 - mae: 3.3599 - val_loss: 17.8634 - val_mae: 3.4404\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.3001 - mae: 3.3222 - val_loss: 14.2371 - val_mae: 2.9727\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.3092 - mae: 3.5959 - val_loss: 11.8417 - val_mae: 2.6183\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.0535 - mae: 3.2971 - val_loss: 13.8712 - val_mae: 2.8502\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.4929 - mae: 3.3596 - val_loss: 13.9534 - val_mae: 2.8264\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.2387 - mae: 3.4566 - val_loss: 15.5836 - val_mae: 2.9845\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.8811 - mae: 3.4245 - val_loss: 14.7719 - val_mae: 2.8216\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.9228 - mae: 3.4871 - val_loss: 16.3308 - val_mae: 2.9666\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.2292 - mae: 3.4830 - val_loss: 13.5951 - val_mae: 2.7863\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.0123 - mae: 3.3799 - val_loss: 17.9251 - val_mae: 3.0984\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 22.9625 - mae: 3.5610 - val_loss: 12.1624 - val_mae: 2.5801\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.3806 - mae: 3.2987 - val_loss: 13.7476 - val_mae: 2.7586\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.9384 - mae: 3.4870 - val_loss: 15.3372 - val_mae: 2.9126\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.9684 - mae: 3.2697 - val_loss: 17.4964 - val_mae: 3.0872\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 25.8285 - mae: 3.4155 - val_loss: 18.4814 - val_mae: 3.2766\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.4104 - mae: 3.3902 - val_loss: 14.0929 - val_mae: 2.6585\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.3763 - mae: 3.4773 - val_loss: 14.8241 - val_mae: 2.7881\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 19.9347 - mae: 3.0979 - val_loss: 17.9764 - val_mae: 3.2382\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.1444 - mae: 3.4461 - val_loss: 15.2810 - val_mae: 2.9351\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.9607 - mae: 3.4163 - val_loss: 14.2018 - val_mae: 2.7851\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.1832 - mae: 3.0601 - val_loss: 13.9922 - val_mae: 2.7984\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.7438 - mae: 3.1574 - val_loss: 16.8209 - val_mae: 3.0953\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.6830 - mae: 3.2619 - val_loss: 12.5149 - val_mae: 2.6260\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 23.3399 - mae: 3.6456 - val_loss: 11.0103 - val_mae: 2.5199\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 19.4072 - mae: 3.2627 - val_loss: 17.0341 - val_mae: 3.1936\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.8265 - mae: 3.1930 - val_loss: 15.4964 - val_mae: 2.9185\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.0405 - mae: 3.3723 - val_loss: 11.9596 - val_mae: 2.6198\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.3559 - mae: 3.1260 - val_loss: 11.3252 - val_mae: 2.5294\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.7174 - mae: 3.1340 - val_loss: 24.3680 - val_mae: 3.8474\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.9054 - mae: 3.3509 - val_loss: 15.3298 - val_mae: 3.1804\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.5011 - mae: 3.4070 - val_loss: 10.3869 - val_mae: 2.3633\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 26.5077 - mae: 3.6302 - val_loss: 11.2665 - val_mae: 2.4744\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 22.0152 - mae: 3.3016 - val_loss: 16.0105 - val_mae: 2.7789\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.1817 - mae: 3.0552 - val_loss: 11.5018 - val_mae: 2.4724\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 19.2067 - mae: 3.1996 - val_loss: 10.6713 - val_mae: 2.4249\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 19.4284 - mae: 3.1951 - val_loss: 12.2284 - val_mae: 2.5807\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 20.2589 - mae: 3.3344 - val_loss: 16.3466 - val_mae: 3.1163\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.1142 - mae: 3.4193 - val_loss: 10.4764 - val_mae: 2.4256\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 19.8378 - mae: 3.2495 - val_loss: 12.7857 - val_mae: 2.6555\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.3287 - mae: 3.1388 - val_loss: 10.8047 - val_mae: 2.4526\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.0755 - mae: 3.0733 - val_loss: 9.6013 - val_mae: 2.3599\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.8769 - mae: 3.1155 - val_loss: 14.6277 - val_mae: 2.9392\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.8904 - mae: 2.9983 - val_loss: 9.9083 - val_mae: 2.3165\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 16.7360 - mae: 3.0748 - val_loss: 10.3578 - val_mae: 2.4394\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.1595 - mae: 3.2571 - val_loss: 10.4579 - val_mae: 2.4424\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 22.4537 - mae: 3.3293 - val_loss: 16.8215 - val_mae: 3.0595\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.1322 - mae: 2.9690 - val_loss: 12.3434 - val_mae: 2.5608\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.7049 - mae: 3.3175 - val_loss: 14.0667 - val_mae: 2.8451\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 15.5471 - mae: 3.0810 - val_loss: 11.6931 - val_mae: 2.5467\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 16.6944 - mae: 3.0411 - val_loss: 11.8780 - val_mae: 2.6939\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.8910 - mae: 2.9811 - val_loss: 11.1711 - val_mae: 2.5142\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 16.4155 - mae: 2.9820 - val_loss: 14.4026 - val_mae: 2.8768\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 21.4464 - mae: 3.2030 - val_loss: 18.0041 - val_mae: 3.0959\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 22.5616 - mae: 3.3869 - val_loss: 11.3376 - val_mae: 2.5131\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.8836 - mae: 3.0822 - val_loss: 14.0275 - val_mae: 2.8284\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.7598 - mae: 2.9106 - val_loss: 10.1880 - val_mae: 2.3687\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.4480 - mae: 3.3231 - val_loss: 16.3101 - val_mae: 3.0874\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.7546 - mae: 3.0499 - val_loss: 12.6090 - val_mae: 2.6147\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.9108 - mae: 3.0996 - val_loss: 12.3958 - val_mae: 2.6725\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 16.6974 - mae: 3.0018 - val_loss: 14.9551 - val_mae: 3.0188\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 17.6260 - mae: 3.1801 - val_loss: 12.3455 - val_mae: 2.7048\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.9120 - mae: 2.8770 - val_loss: 14.4570 - val_mae: 2.7853\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.0670 - mae: 3.1184 - val_loss: 16.2873 - val_mae: 2.9737\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.5299 - mae: 3.3209 - val_loss: 18.0250 - val_mae: 3.1917\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 20.2853 - mae: 3.3299 - val_loss: 14.7007 - val_mae: 2.8320\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 17.9964 - mae: 3.1288 - val_loss: 16.3582 - val_mae: 2.9543\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.3340 - mae: 2.9904 - val_loss: 12.6276 - val_mae: 2.6160\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.3374 - mae: 3.1696 - val_loss: 13.0503 - val_mae: 2.6969\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.2788 - mae: 2.8764 - val_loss: 11.2937 - val_mae: 2.4537\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.4814 - mae: 2.9616 - val_loss: 9.9632 - val_mae: 2.3571\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 16.6717 - mae: 2.9443 - val_loss: 12.4389 - val_mae: 2.6471\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.7425 - mae: 3.1698 - val_loss: 14.0077 - val_mae: 2.8159\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.8020 - mae: 2.9788 - val_loss: 9.8152 - val_mae: 2.3326\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.7826 - mae: 3.0724 - val_loss: 12.8691 - val_mae: 2.7012\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 18.4860 - mae: 3.0884 - val_loss: 19.5456 - val_mae: 3.2522\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.1282 - mae: 3.0956 - val_loss: 15.9991 - val_mae: 2.9617\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.0981 - mae: 2.8894 - val_loss: 18.2997 - val_mae: 3.1753\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.3888 - mae: 3.1057 - val_loss: 14.1696 - val_mae: 2.8364\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.3555 - mae: 3.0827 - val_loss: 12.2286 - val_mae: 2.6327\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 19.7870 - mae: 3.1422 - val_loss: 9.7377 - val_mae: 2.3948\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.5471 - mae: 3.1719 - val_loss: 12.7331 - val_mae: 2.7402\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.3084 - mae: 3.0095 - val_loss: 12.7307 - val_mae: 2.6167\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.1721 - mae: 2.9801 - val_loss: 13.0551 - val_mae: 2.6577\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.8833 - mae: 3.1926 - val_loss: 10.3469 - val_mae: 2.4086\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.2834 - mae: 2.8358 - val_loss: 10.3475 - val_mae: 2.4290\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.4944 - mae: 2.9496 - val_loss: 11.0281 - val_mae: 2.4589\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 18.7561 - mae: 3.0500 - val_loss: 24.2008 - val_mae: 3.7953\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.0882 - mae: 3.1195 - val_loss: 10.4116 - val_mae: 2.4692\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.0131 - mae: 3.1293 - val_loss: 13.1778 - val_mae: 2.7916\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.7934 - mae: 2.9703 - val_loss: 7.7687 - val_mae: 2.1658\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.4012 - mae: 3.0922 - val_loss: 10.7843 - val_mae: 2.5558\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.1354 - mae: 2.9558 - val_loss: 9.1183 - val_mae: 2.2713\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.3484 - mae: 3.0156 - val_loss: 13.6490 - val_mae: 2.7236\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.9154 - mae: 2.8839 - val_loss: 9.1661 - val_mae: 2.2898\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 15.9808 - mae: 2.8549 - val_loss: 11.2658 - val_mae: 2.5498\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.1788 - mae: 3.2211 - val_loss: 15.2766 - val_mae: 2.9468\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.1589 - mae: 2.9611 - val_loss: 9.3589 - val_mae: 2.2871\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 13.5145 - mae: 2.7478 - val_loss: 10.2151 - val_mae: 2.4213\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 16.8287 - mae: 3.0554 - val_loss: 12.9240 - val_mae: 2.7103\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.3316 - mae: 2.8738 - val_loss: 14.8430 - val_mae: 2.8501\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.6583 - mae: 3.0914 - val_loss: 14.8052 - val_mae: 2.9504\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.0523 - mae: 3.0071 - val_loss: 16.7452 - val_mae: 3.0989\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 18.7169 - mae: 3.1215 - val_loss: 11.3975 - val_mae: 2.5155\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.7077 - mae: 2.9710 - val_loss: 14.8175 - val_mae: 2.8749\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 18.5679 - mae: 3.1975 - val_loss: 14.9263 - val_mae: 2.7679\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.7087 - mae: 3.0145 - val_loss: 10.0778 - val_mae: 2.3070\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.2163 - mae: 3.0337 - val_loss: 9.6665 - val_mae: 2.3345\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 18.6683 - mae: 3.1978 - val_loss: 25.4262 - val_mae: 3.8302\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 20.8936 - mae: 3.2422 - val_loss: 13.2307 - val_mae: 2.7301\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.0314 - mae: 2.8898 - val_loss: 11.6824 - val_mae: 2.5865\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 18.9114 - mae: 2.9579 - val_loss: 11.4651 - val_mae: 2.5211\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 21.2988 - mae: 3.3105 - val_loss: 12.0582 - val_mae: 2.4136\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c2262c050>"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "friendly-powell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "friendly-powell",
        "outputId": "78d37edb-1365-47d0-ac8c-93df6400c6aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 15.9561 - mae: 2.8549\n",
            "Test Loss: [15.95609188079834, 2.8548665046691895]\n"
          ]
        }
      ],
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "british-vegetation",
      "metadata": {
        "id": "british-vegetation"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "## Cuestión 3: Utilice el mismo modelo de la cuestión anterior pero añadiendo un callback de early stopping. Obtenga un test loss inferior al del modelo anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "precise-finish",
      "metadata": {
        "id": "precise-finish"
      },
      "outputs": [],
      "source": [
        "model2 = tf.keras.models.Sequential()\n",
        "\n",
        "kernel_regularizer_l2 = keras.regularizers.l2(5e-4)\n",
        "\n",
        "## Hidden Layer + regularization\n",
        "model2.add(keras.layers.Dense(100, input_shape=(13, ), activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_1'))\n",
        "\n",
        "model2.add(layers.Dropout(0.3))\n",
        "\n",
        "model2.add(layers.Dense(90, activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_2'))\n",
        "\n",
        "model2.add(layers.Dropout(0.3))\n",
        "\n",
        "model2.add(layers.Dense(80, activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_3'))\n",
        "\n",
        "model2.add(layers.Dropout(0.3))\n",
        "\n",
        "model2.add(layers.Dense(70, activation='relu',kernel_regularizer=kernel_regularizer_l2, name='hidden_layer_4'))\n",
        "\n",
        "model2.add(layers.Dropout(0.3))\n",
        "\n",
        "### output layer\n",
        "model2.add(layers.Dense(1, activation='linear', name='layer_output'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "blond-telephone",
      "metadata": {
        "id": "blond-telephone"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo\n",
        "\n",
        "model2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "id": "subsequent-roads",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "subsequent-roads",
        "outputId": "d0e4c2aa-d1a6-4289-97bc-96040052ccbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 14.6009 - mae: 2.8149 - val_loss: 12.6984 - val_mae: 2.6821\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 15.6232 - mae: 2.8195 - val_loss: 9.6795 - val_mae: 2.3627\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 14.0636 - mae: 2.6088 - val_loss: 11.9128 - val_mae: 2.5726\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 15.5735 - mae: 2.8398 - val_loss: 10.5988 - val_mae: 2.4156\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 13.5140 - mae: 2.7925 - val_loss: 9.2715 - val_mae: 2.1967\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 12.1406 - mae: 2.5564 - val_loss: 11.0632 - val_mae: 2.3719\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 13.8888 - mae: 2.8345 - val_loss: 11.2699 - val_mae: 2.4598\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 15.5508 - mae: 2.8058 - val_loss: 13.5892 - val_mae: 2.6557\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 12.4847 - mae: 2.6032 - val_loss: 11.7659 - val_mae: 2.4223\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 15.4038 - mae: 2.8410 - val_loss: 12.5105 - val_mae: 2.5401\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.9688 - mae: 2.6669 - val_loss: 13.4723 - val_mae: 2.6736\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.0325 - mae: 2.6460 - val_loss: 10.2426 - val_mae: 2.2736\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.3496 - mae: 2.9309 - val_loss: 9.5537 - val_mae: 2.2233\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.3558 - mae: 2.8854 - val_loss: 10.3957 - val_mae: 2.3604\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 11.9261 - mae: 2.6813 - val_loss: 9.6394 - val_mae: 2.2016\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 13.4515 - mae: 2.6897 - val_loss: 14.1549 - val_mae: 2.8705\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.5616 - mae: 2.7714 - val_loss: 11.7564 - val_mae: 2.5498\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.9892 - mae: 2.7177 - val_loss: 9.9852 - val_mae: 2.3253\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.4501 - mae: 2.6710 - val_loss: 17.1162 - val_mae: 3.1647\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.6660 - mae: 2.8468 - val_loss: 8.9944 - val_mae: 2.1932\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.9618 - mae: 2.8897 - val_loss: 16.1450 - val_mae: 3.0779\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.0315 - mae: 2.6568 - val_loss: 8.1730 - val_mae: 2.1236\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.6790 - mae: 2.8126 - val_loss: 11.3574 - val_mae: 2.5722\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.7234 - mae: 2.8601 - val_loss: 15.3810 - val_mae: 2.9469\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 15.5822 - mae: 2.8854 - val_loss: 11.2301 - val_mae: 2.3565\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.2322 - mae: 2.7410 - val_loss: 12.5793 - val_mae: 2.4973\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.3342 - mae: 2.7528 - val_loss: 12.3916 - val_mae: 2.3998\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.9878 - mae: 2.8625 - val_loss: 11.5836 - val_mae: 2.2944\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 12.5705 - mae: 2.6554 - val_loss: 9.9878 - val_mae: 2.2218\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.1135 - mae: 2.7294 - val_loss: 11.2793 - val_mae: 2.4122\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.0504 - mae: 2.6984 - val_loss: 15.8044 - val_mae: 2.9771\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.1219 - mae: 2.9514 - val_loss: 10.8887 - val_mae: 2.2699\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.2426 - mae: 2.8190 - val_loss: 12.1952 - val_mae: 2.3556\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 10.8693 - mae: 2.4694 - val_loss: 12.6855 - val_mae: 2.4303\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 12.5344 - mae: 2.7123 - val_loss: 15.1453 - val_mae: 2.7281\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.3288 - mae: 2.7892 - val_loss: 11.1061 - val_mae: 2.3032\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.5908 - mae: 2.7445 - val_loss: 9.2654 - val_mae: 2.2274\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 12.0597 - mae: 2.5986 - val_loss: 10.1633 - val_mae: 2.2835\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 13.0111 - mae: 2.6190 - val_loss: 11.2574 - val_mae: 2.3998\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.8363 - mae: 2.7862 - val_loss: 12.1868 - val_mae: 2.5217\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.6654 - mae: 2.7457 - val_loss: 11.5956 - val_mae: 2.4176\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 15.1390 - mae: 2.7357 - val_loss: 9.8000 - val_mae: 2.3116\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 16.5341 - mae: 2.9034 - val_loss: 8.9221 - val_mae: 2.1600\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 9.9270 - mae: 2.3678 - val_loss: 12.5582 - val_mae: 2.5934\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 12.1629 - mae: 2.6359 - val_loss: 10.0310 - val_mae: 2.2596\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.8267 - mae: 2.7907 - val_loss: 8.4519 - val_mae: 2.1419\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 12.4442 - mae: 2.7021 - val_loss: 11.9270 - val_mae: 2.5204\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.6551 - mae: 2.8921 - val_loss: 10.5187 - val_mae: 2.3471\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.8058 - mae: 2.7488 - val_loss: 11.1065 - val_mae: 2.3679\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 12.8976 - mae: 2.6198 - val_loss: 11.7083 - val_mae: 2.5629\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.5775 - mae: 2.8160 - val_loss: 9.6108 - val_mae: 2.2856\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.9276 - mae: 2.6927 - val_loss: 8.9645 - val_mae: 2.2648\n",
            "Epoch 52: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c326fb290>"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ],
      "source": [
        "## definir el early stopping callback\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # can be 'val_accuracy'\n",
        "    patience=30,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
        "    verbose=1)\n",
        "\n",
        "model2.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=15,\n",
        "          validation_split=0.2,\n",
        "          verbose=1,\n",
        "          callbacks=[es_callback]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "id": "pressing-object",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pressing-object",
        "outputId": "a34c502f-d89f-4bc1-b506-cc6f6415004c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 14.0372 - mae: 2.4993\n",
            "Test Loss: [14.037203788757324, 2.499274730682373]\n"
          ]
        }
      ],
      "source": [
        "# No modifique el código\n",
        "results = model2.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "addressed-lesbian",
      "metadata": {
        "id": "addressed-lesbian"
      },
      "source": [
        "<a name='1.4'></a>\n",
        "## Cuestión 4: ¿Podría haberse usado otra función de activación de la neurona de salida? En caso afirmativo especifíquela."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ruled-silicon",
      "metadata": {
        "id": "ruled-silicon"
      },
      "source": [
        "La función ReLu se puede utilizar en capas de salida para regresiones positivas, es decir, regresiones que solo tengan posibilidad de dar valor positivo, como en este caso, predecir el valor de una vivienda. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "robust-christianity",
      "metadata": {
        "id": "robust-christianity"
      },
      "source": [
        "<a name='1.5'></a>\n",
        "## Cuestión 5:  ¿Qué es lo que una neurona calcula?\n",
        "\n",
        "**a)** Una función de activación seguida de una suma ponderada  de las entradas.\n",
        "\n",
        "**b)** Una suma ponderada  de las entradas seguida de una función de activación.\n",
        "\n",
        "**c)** Una función de pérdida, definida sobre el target.\n",
        "\n",
        "**d)** Ninguna  de las anteriores es correcta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "joined-burden",
      "metadata": {
        "id": "joined-burden"
      },
      "source": [
        "**b) Una suma ponderada de las entradas seguida de una función de activación.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iraqi-european",
      "metadata": {
        "id": "iraqi-european"
      },
      "source": [
        "<a name='1.6'></a>\n",
        "## Cuestión 6:  ¿Cuál de estas funciones de activación no debería usarse en una capa oculta (hidden layer)?\n",
        "\n",
        "**a)** `sigmoid`\n",
        "\n",
        "**b)** `tanh`\n",
        "\n",
        "**c)** `relu`\n",
        "\n",
        "**d)** `linear`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cardiovascular-attack",
      "metadata": {
        "id": "cardiovascular-attack"
      },
      "source": [
        "**d) linear**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ranging-utilization",
      "metadata": {
        "id": "ranging-utilization"
      },
      "source": [
        "<a name='1.7'></a>\n",
        "## Cuestión 7:  ¿Cuál de estas técnicas es efectiva para combatir el overfitting en una red con varias capas ocultas? Ponga todas las que lo sean.\n",
        "\n",
        "**a)** Dropout\n",
        "\n",
        "**b)** Regularización L2.\n",
        "\n",
        "**c)** Aumentar el tamaño del test set.\n",
        "\n",
        "**d)** Aumentar el tamaño del validation set.\n",
        "\n",
        "**e)** Reducir el número de capas de la red.\n",
        "\n",
        "**f)** Data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessible-trainer",
      "metadata": {
        "id": "accessible-trainer"
      },
      "source": [
        "**a) Dropout**\n",
        "\n",
        "**b) Regularización L2.**\n",
        "\n",
        "**c) Aumentar el tamaño del test set.**\n",
        "\n",
        "**d) Aumentar el tamaño del validation set.**\n",
        "\n",
        "**e) Reducir el número de capas de la red.**\n",
        "\n",
        "**f) Data augmentation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "recreational-deposit",
      "metadata": {
        "id": "recreational-deposit"
      },
      "source": [
        "<a name='1.8'></a>\n",
        "## Cuestión 8:  Supongamos que queremos entrenar una red para un problema de clasificación de imágenes con las siguientes clases: {'perro','gato','persona'}. ¿Cuántas neuronas y que función de activación debería tener la capa de salida? ¿Qué función de pérdida (loss function) debería usarse?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confirmed-roulette",
      "metadata": {
        "id": "confirmed-roulette"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "painful-decade",
      "metadata": {
        "id": "painful-decade"
      },
      "source": [
        "<a name='actividad_2'></a>\n",
        "# Actividad 2: Redes Convolucionales\n",
        "\n",
        "Vamos a usar el dataset [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html), que son 60000 imágenes de 32x32 a color  con 10 clases diferentes. Para realizar mejor la práctica puede consultar [Introduction_to_CNN.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Introduction_to_CNN.ipynb).\n",
        "\n",
        "\n",
        "\n",
        "**Puntuación**: \n",
        "\n",
        "- [Cuestión 1](#2.1): 1 pt\n",
        "- [Cuestión 2](#2.2): 1.5 pt\n",
        "- [Cuestión 3](#2.3): 0.5 pts\n",
        "- [Cuestión 4](#2.4): 0.5 pts\n",
        "- [Cuestión 5](#2.5): 0.5 pts\n",
        "- [Cuestión 6](#2.6): 0.5 pts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Puede normalizar las imágenes al principio o usar la capa [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling):\n",
        "\n",
        "```python\n",
        "tf.keras.layers.experimental.preprocessing.Rescaling(\n",
        "    scale, offset=0.0, name=None, **kwargs\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorporate-terrorist",
      "metadata": {
        "id": "incorporate-terrorist"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brazilian-rhythm",
      "metadata": {
        "id": "brazilian-rhythm"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i])\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extreme-quantum",
      "metadata": {
        "id": "extreme-quantum"
      },
      "outputs": [],
      "source": [
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "living-philosophy",
      "metadata": {
        "id": "living-philosophy"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "## Cuestión 1: Cree una red convolucional con la API funcional con al menos dos capas convolucionales y al menos dos capas de pooling. Utilize sólo [Average Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) y no añada ninguna regularización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "atmospheric-sight",
      "metadata": {
        "id": "atmospheric-sight"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "needed-arena",
      "metadata": {
        "id": "needed-arena"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pursuant-paper",
      "metadata": {
        "id": "pursuant-paper"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=25, batch_size=64,\n",
        "                    validation_split=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applicable-honduras",
      "metadata": {
        "id": "applicable-honduras"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "numerous-invite",
      "metadata": {
        "id": "numerous-invite"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "## Cuestión 2: Cree un modelo con la API funcional con un máximo de 2 capas convolucionales y un máximo de 2 capas de pooling. Utilize  [Max Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) o [Average Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) y  añada la regularización que quiera. Debe obtener un `Test accuracy > 0.68`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "annual-diploma",
      "metadata": {
        "id": "annual-diploma"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indian-messaging",
      "metadata": {
        "id": "indian-messaging"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "functional-republic",
      "metadata": {
        "id": "functional-republic"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=..., batch_size=...,\n",
        "                    validation_split=0.15, callbacks=lbacks=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorrect-completion",
      "metadata": {
        "id": "incorrect-completion"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "optical-arizona",
      "metadata": {
        "id": "optical-arizona"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "## Cuestión 3: Añada data augmentation al principio del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "previous-boxing",
      "metadata": {
        "id": "previous-boxing"
      },
      "outputs": [],
      "source": [
        "data_augmentation=... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comprehensive-directive",
      "metadata": {
        "id": "comprehensive-directive"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "data_aug= ...\n",
        "\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "statutory-covering",
      "metadata": {
        "id": "statutory-covering"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "western-energy",
      "metadata": {
        "id": "western-energy"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=..., batch_size=...,\n",
        "                    validation_split=0.15, callbacks=lbacks=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classical-charm",
      "metadata": {
        "id": "classical-charm"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sweet-implement",
      "metadata": {
        "id": "sweet-implement"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "## Cuestión 4: Cree el mismo  modelo de manera secuencial. No es necesario compilar ni entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auburn-lawrence",
      "metadata": {
        "id": "auburn-lawrence"
      },
      "outputs": [],
      "source": [
        "model_seq = tf.keras.models.Sequential()\n",
        "# Código aquí\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "present-consortium",
      "metadata": {
        "id": "present-consortium"
      },
      "source": [
        "<a name='2.5'></a>\n",
        "## Cuestión 5: Si tenenemos una  una imagen de entrada de 300 x 300 a color (RGB) y queremos usar una red densa. Si la primera capa oculta tiene 100 neuronas, ¿Cuántos parámetros tendrá esa capa (sin incluir el bias) ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "novel-calcium",
      "metadata": {
        "id": "novel-calcium"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "complicated-positive",
      "metadata": {
        "id": "complicated-positive"
      },
      "source": [
        "<a name='2.6'></a>\n",
        "## Cuestión 6   Ponga  las verdaderas ventajas de las redes convolucionales respecto a las densas\n",
        "\n",
        "**a)** Reducen el número total de parámetros, reduciendo así el overfitting.\n",
        "\n",
        "**b)** Permiten utilizar una misma 'función'  en varias localizaciones de la imagen de entrada, en lugar de aprender una función diferente para cada pixel.\n",
        "\n",
        "**c)** Permiten el uso del transfer learning.\n",
        "\n",
        "**d)** Generalmente son menos profundas, lo que facilita su entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dirty-nirvana",
      "metadata": {
        "id": "dirty-nirvana"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "name": "Tarea.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}